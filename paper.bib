@article{lda,
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
title = {Latent Dirichlet Allocation},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {993–1022},
numpages = {30}
}

@inproceedings{ClusterPageRank,
author = {Wan, Xiaojun and Yang, Jianwu},
title = {Multi-Document Summarization Using Cluster-Based Link Analysis},
year = {2008},
isbn = {9781605581644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390334.1390386},
doi = {10.1145/1390334.1390386},
abstract = {The Markov Random Walk model has been recently exploited for multi-document summarization by making use of the link relationships between sentences in the document set, under the assumption that all the sentences are indistinguishable from each other. However, a given document set usually covers a few topic themes with each theme represented by a cluster of sentences. The topic themes are usually not equally important and the sentences in an important theme cluster are deemed more salient than the sentences in a trivial theme cluster. This paper proposes the Cluster-based Conditional Markov Random Walk Model (ClusterCMRW) and the Cluster-based HITS Model (ClusterHITS) to fully leverage the cluster-level information. Experimental results on the DUC2001 and DUC2002 datasets demonstrate the good effectiveness of our proposed summarization models. The results also demonstrate that the ClusterCMRW model is more robust than the ClusterHITS model, with respect to different cluster numbers.},
booktitle = {Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {299–306},
numpages = {8},
keywords = {multi-document summarization, conditional Markov random walk model, cluster-based link analysis, HITS},
location = {Singapore, Singapore},
series = {SIGIR '08}
}

@INPROCEEDINGS{Tang2008,
  author={J. {Tang} and R. {Jin} and J. {Zhang}},
  booktitle={2008 Eighth IEEE International Conference on Data Mining}, 
  title={A Topic Modeling Approach and Its Integration into the Random Walk Framework for Academic Search}, 
  year={2008},
  volume={},
  number={},
  pages={1055-1060},}


@article{quanti,
	author = {Jacobi, Carina and Atteveldt, Wouter and Welbers, Kasper},
	year = {2015},
	month = {10},
	pages = {1-18},
	title = {Quantitative analysis of large amounts of journalistic texts using topic modelling},
	volume = {4},
	journal = {Digital Journalism},
	doi = {10.1080/21670811.2015.1093271}
}

@article{google_pagerank2006,
  title={The effect of new links on Google PageRank},
  author={Avrachenkov, Konstantin and Litvak, Nelly},
  journal={Stochastic Models},
  volume={22},
  number={2},
  pages={319--331},
  year={2006},
  publisher={Taylor \& Francis}
}


@techreport{pagerank_1999,
          number = {1999-66},
           month = {November},
          author = {Lawrence Page and Sergey Brin and Rajeev Motwani and Terry Winograd},
            note = {Previous number = SIDL-WP-1999-0120},
           title = {The PageRank Citation Ranking: Bringing Order to the Web.},
            type = {Technical Report},
       publisher = {Stanford InfoLab},
            year = {1999},
     institution = {Stanford InfoLab},
             url = {http://ilpubs.stanford.edu:8090/422/},
        abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.}
}


@article{jelodar2019latent,
  title={Latent Dirichlet Allocation (LDA) and Topic modeling: models, applications, a survey},
  author={Jelodar, Hamed and Wang, Yongli and Yuan, Chi and Feng, Xia and Jiang, Xiahui and Li, Yanchao and Zhao, Liang},
  journal={Multimedia Tools and Applications},
  volume={78},
  number={11},
  pages={15169--15211},
  year={2019},
  publisher={Springer}
}

@article{blei2012topicmodels,
	author = {Blei, David M.},
	title = {Probabilistic Topic Models},
	year = {2012},
	issue_date = {April 2012},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {55},
	number = {4},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/2133806.2133826},
	doi = {10.1145/2133806.2133826},
	abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
	journal = {Commun. ACM},
	month = apr,
	pages = {77–84},
	numpages = {8}
}

@article{jensen-shannon2003,
  title={A new metric for probability distributions},
  author={Endres, Dominik Maria and Schindelin, Johannes E},
  journal={IEEE Transactions on Information theory},
  volume={49},
  number={7},
  pages={1858--1860},
  year={2003},
  publisher={IEEE}
}


@inproceedings{jensen-shannondis2003,
author="Connor, Richard
and Cardillo, Franco Alberto
and Moss, Robert
and Rabitti, Fausto",
editor="Brisaboa, Nieves
and Pedreira, Oscar
and Zezula, Pavel",
title="Evaluation of Jensen-Shannon Distance over Sparse Data",
booktitle="Similarity Search and Applications",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="163--168",
isbn="978-3-642-41062-8"
}

@article{Pedersen2009DanNetTC,
  title={DanNet: the challenge of compiling a wordnet for Danish by reusing a monolingual dictionary},
  author={Bolette S. Pedersen and Sanni Nimb and J. Asmussen and N. H. S{\o}rensen and L. Trap-Jensen and H. Lorentzen},
  journal={Language Resources and Evaluation},
  year={2009},
  volume={43},
  pages={269-299}
}


@inproceedings{blei2006dynamic,
	author = {Blei, David M. and Lafferty, John D.},
	title = {Dynamic Topic Models},
	year = {2006},
	isbn = {1595933832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1143844.1143859},
	doi = {10.1145/1143844.1143859},
	abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.},
	booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
	pages = {113–120},
	numpages = {8},
	location = {Pittsburgh, Pennsylvania, USA},
	series = {ICML '06}
}

@article{blei2007correlated,
	title={A correlated topic model of Science},
	volume={1},
	ISSN={1932-6157},
	url={http://dx.doi.org/10.1214/07-AOAS114},
	DOI={10.1214/07-aoas114},
	number={1},
	journal={The Annals of Applied Statistics},
	publisher={Institute of Mathematical Statistics},
	author={Blei, David M. and Lafferty, John D.},
	year={2007},
	month={Jun},
	pages={17–35}
}

@incollection{yang2009topic,
  title={Topic-level random walk through probabilistic model},
  author={Yang, Zi and Tang, Jie and Zhang, Jing and Li, Juanzi and Gao, Bo},
  booktitle={Advances in Data and Web Management},
  pages={162--173},
  year={2009},
  publisher={Springer}
}

@article{bm251996,
  title={Okapi at TREC-4},
  author={Robertson, Stephen E and Walker, Steve and Beaulieu, MM and Gatford, Mike and Payne, Alison},
  journal={Nist Special Publication Sp},
  pages={73--96},
  year={1996},
  publisher={NATIONAL INSTIUTE OF STANDARDS \& TECHNOLOGY}
}


@inproceedings{li2006pachinko,
	author = {Li, Wei and McCallum, Andrew},
	title = {Pachinko Allocation: DAG-Structured Mixture Models of Topic Correlations},
	year = {2006},
	isbn = {1595933832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1143844.1143917},
	doi = {10.1145/1143844.1143917},
	abstract = {Latent Dirichlet allocation (LDA) and other related topic models are increasingly popular tools for summarization and manifold discovery in discrete data. However, LDA does not capture correlations between topics. In this paper, we introduce the pachinko allocation model (PAM), which captures arbitrary, nested, and possibly sparse correlations between topics using a directed acyclic graph (DAG). The leaves of the DAG represent individual words in the vocabulary, while each interior node represents a correlation among its children, which may be words or other interior nodes (topics). PAM provides a flexible alternative to recent work by Blei and Lafferty (2006), which captures correlations only between pairs of topics. Using text data from newsgroups, historic NIPS proceedings and other research paper corpora, we show improved performance of PAM in document classification, likelihood of held-out data, the ability to support finer-grained topics, and topical keyword coherence.},
	booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
	pages = {577–584},
	numpages = {8},
	location = {Pittsburgh, Pennsylvania, USA},
	series = {ICML '06}
}

@article{quanti,
	author = {Jacobi, Carina and Atteveldt, Wouter and Welbers, Kasper},
	year = {2015},
	month = {10},
	pages = {1-18},
	title = {Quantitative analysis of large amounts of journalistic texts using topic modelling},
	volume = {4},
	journal = {Digital Journalism},
	doi = {10.1080/21670811.2015.1093271}
}
