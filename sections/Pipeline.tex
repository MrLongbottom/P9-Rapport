\section{Method}\label{sec:method}
This section describes the steps of our method on an abstract level.
\todo{ref pipelinefigure} visualizes these steps as a flowchart.
Each phase of the method is described later on in more detail. 

We start with a dataset, in our case consisting of news articles from the media group Nordjyske. 
Their primary focus is to maintain a variety of local newspapers within the North Jutland region of Denmark. 
The data ranges from 2017, where a total of $\sim$~63.000 articles have been extracted from their database.

\subsection*{Step 1: Preprocessing Phase}
This phase applies different \gls{NLP} methods, such as stemming and removing stop words, to simplify the data set and remove redundant information.
Details of this phase are given in \autoref{sec:prepro}.
After finishing this phase, we are left with $\sim$~32.000 articles, which will be used in the next step.

\subsection*{Step 2: LDA}
We train a \acrfull{lda} model on the dataset to generate topics based on the content of articles within the dataset. 
We describe the investigation and selection of hyper parameters in \autoref{subsec:hyperparameters}. 
After the model has been trained, we have a document-topic distribution matrix $\theta$ and a topic-word distribution matrix $\beta$.

\subsection*{Step 3: Query Preprocessing}
A search query, consisting of words, is stemmed and turned into a list of words.
For each word, we generate a topic distribution based on topic-word distribution $\beta$ if the model knows the words within the query.
Otherwise, we let the model predict which topics the unknown words belong to.
We use the final distribution, as a personalization vector for our PageRank combination models.
\todo[inline]{Check that the description of how the personalization vector is made, is correct}
The output of this step is a list of search queries.


\subsection*{Step 4: Evaluation of models}
In this phase, we evaluate different combinations of models using the evaluation metrics described in \todo{ref evaluation metric section}.
The evaluation is done by testing the search queries against the different models. 
The combinations and results can be seen in section \todo{ref combination section } and \todo{ref result section }, respectively.


\input{figures/pipeline_figure.tex}
