\subsection{Project Process}
In this section, we provide a general overview of the project, as well as some of the changes and challenges we underwent during the project.
Below, is a brief overview of the points covered in this section:

\begin{itemize}
	\item Initial Idea - Combine LDA and PageRank
	\item Preprocessing \& LDA implementation
	\item Adjacency matrix construction
	\begin{itemize}
		\item Challenges with calculation time
		\item Focus on more restrictive preprocessing and thresholds
	\end{itemize}
	\item Bad results
	\item Discovery of related papers
	\item Found out our model didn't converge
\end{itemize}

Each of these items will now be described in more detail.

Our initial idea with the project was to combine \gls{lda} with a PageRank algorithm to see if this could improve the \acrlong{ir} results of \gls{lda}.
Originally, we wanted to use the Clustering PageRank method as described in \citeauthor{ClusterPageRank}\cite{ClusterPageRank}, but that idea was later scrapped.

We began by implementing some initial preprocessing methods and used an \gls{lda} implementation from Gensim\footnote{\url{https://radimrehurek.com/gensim/models/ldamodel.html}}.

After this, we only needed to combine \gls{lda} with a PageRank implementation. 
But to do so, we first needed to construct an adjacency matrix.
This is where we started to run into trouble as constructing a large adjacency matrix based on a similarity measure took an extremely long time, which forced us to make changes to the way we constructed the adjacency matrix as well as changes to the earlier phases: preprocessing and \gls{lda}.
We made the preprocessing more aggressive and implemented more preprocessing methods, to continually make the dataset smaller while keeping the relevant data.

Importantly, we also implemented thresholds to the topic-document distribution matrix $\theta$ and the topic-word distribution matrix $\beta$ made by the \gls{lda} model.
For each distribution we replaced values below one divided by the number of values in the distribution, with 0.
This was done to make sure that every document wasn't connected to every other document in the adjacency matrix, and to save calculation time by working with sparse matrices.
This change ended up causing a lot more problems later on, as many future calculations expected non-zero values, and would be dominated by zeros because of multiplication.

Lastly, we also tried many different ways to construct the adjacency matrix more efficiently, described further in \autoref{app:adj_matrix}.

However even after we managed to produce an adjacency matrix our results proved to be unrealistically bad, pointing towards something being wrong with our implementation.
From here we began searching for more similar papers, where we found: \todo{refs}.
We focused particularly on \todo{ref}, due to a strong similarity between their project and our ideas.
We used their methods to improve upon several parts of our methods, and we changed from building a single framework, to testing and comparing multiple different methods. 
However, our results were still bad.

It was by then that we began testing our \gls{lda} model and found out that it was unable to converge using only a single pass through the corpus. 
We then began testing hyperparameters for the implementation of \gls{lda}, before once again finding the optimal hyperparameters for our dataset.
