\subsection{Project Process}
\begin{itemize}
	\item Initial Idea - Combine LDA and PageRank (clustering)
	\item Preprocessing \& LDA implementation
	\item Adjacency matrix construction
	\begin{itemize}
		\item challenges with calculation time
		\item focus on more restricting preprocessing and thresholds
	\end{itemize}
	\item Bad results
	\item Discovery of related papers
	\item Found out our model didn't converge
\end{itemize}

Our initial idea with the project was to combine \gls{lda} with a PageRank algorithm to see if this could improve the information retrieval results of \gls{lda}.

We began by implementing some initial preprocessing methods and using an \gls{lda} implementation by gensim\footnote{https://radimrehurek.com/gensim/models/ldamodel.html}.

After this, we only needed to combine \gls{lda} with a PageRank implementation. 
But to do so, we first needed to construct an adjacency matrix. 
This is where we started to run into trouble as constructing a large adjacency matrix based on a similarity measure took an extremely long time, which forced us to make changes to the way we constructed the adjacency matrix as well as changes to the earlier phases: preprocessing and \gls{lda}.
We made the preprocessing more aggressive and implemented more preprocessing methods, to continually make the dataset smaller while keeping the relevant data.
We also implemented thresholds to the $\theta$ and $beta$ matrixes made by the \gls{lda} model. 
All values under the normal distribution values were set to 0 so that we could work more efficiently with sparse matrixes.
This ended up causing more trouble than it was worth, as we now had to deal with a large portion of our evaluations containing no values.
We also tried many different ways to construct the adjacency matrix more efficiently, described further in \autoref{app:adj_matrix}.

However even after we managed to produce an adjacency matrix our results proved to be very bad.
From here we began searching for more similar papers, where we found: \todo{refs}.
We focused particularly on \todo{ref}, due to a strong similarity between their project and our ideas.
We used their methods to improve upon several parts of our methods, and we changed from building a single framework, to testing and comparing multiple different methods. 
However, our results were still bad.

It was by then that we began testing our \gls{lda} model and found out that it was unable to converge using only a single pass through the corpus. 
We then began testing hyperparameters for the implementation of \gls{lda}, before once again finding hyperparameters for our \gls{lda}.
