\subsection{Project Process}
In this section, we provide a general overview of the project, as well as some of the changes and challenges we underwent during the project.
Below, is a brief overview of the points covered in this section:

\begin{itemize}
	\item Initial Idea - Combine LDA and PageRank
	\item Preprocessing \& LDA implementation
	\item Adjacency matrix construction
	\begin{itemize}
		\item Challenges with calculation time
		\item Focus on more restrictive preprocessing and thresholds
	\end{itemize}
	\item Discovery of related papers
	\item Found out our model did not converge
\end{itemize}

Each of these items will now be described in more detail.

Our initial idea with the project was to combine \gls{lda} with a PageRank algorithm to see if this could improve the \acrlong{ir} results of \gls{lda}.
Originally, we wanted to use the Clustering PageRank method as described in \citet{ClusterPageRank}, but that idea was later scrapped.

We began by implementing some initial preprocessing methods and used an \gls{lda} implementation from Gensim\footnote{\url{https://radimrehurek.com/gensim/models/ldamodel.html}}.

After this, we only needed to combine \gls{lda} with a \gls{pr} implementation. 
To do so, we first needed to construct an adjacency matrix.
Though, when constructing a large adjacency matrix based on a similarity measure, it took an extremely long time, which made us evaluate what changes were necessary to the way we constructed the adjacency matrix and the earlier phases: preprocessing and \gls{lda}.
From this, we made the preprocessing more aggressive and implemented more preprocessing methods, to continually make the dataset smaller while keeping the relevant data.

Importantly, we also implemented thresholds to the topic-document distribution matrix $\theta$ and the topic-word distribution matrix $\beta$ made by the \gls{lda} model.
For each distribution we replaced values below one divided by the number of values in the distribution, with 0.
This was done to make sure that every document wasn't connected to every other document in the adjacency matrix, and to save calculation time by working with sparse matrices.
We later learned that future calculations expected non-zero values, and would be dominated by zeros because of multiplication.
This threshold was therefore removed.

Lastly, we also tried many different ways to construct the adjacency matrix more efficiently, described further in \autoref{app:adj_matrix}.

However, even after we managed to produce an adjacency matrix, our results did not perform as well as expected which made us search for possible solutions.
From here we began searching for more similar papers, where we found: \citet{yang2009topic} and \citet{Tang2008}.
We focused particularly on \cite{yang2009topic}, due to a strong similarity between their project and our ideas.
We used their methods to improve upon several parts of our methods, and we changed from building a single framework, to testing and comparing multiple different methods, as they had done.
However, our results were still not as good as expected.

We then learned through tests of our \gls{lda} model that it was unable to converge using only a single pass through the corpus.
We then began testing hyperparameters for the implementation of \gls{lda}, before once again finding the optimal hyperparameters for our dataset.
