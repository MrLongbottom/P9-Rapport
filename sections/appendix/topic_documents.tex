\subsection{Representative Documents for Topics}
In \autoref{tab:representative_documents}, 5 documents that are representative of the 5 topics from \autoref{fig:30TopicWords} can be seen.
In this table, the topic number, document number, the weight of the document, and the document itself are shown.
Here, the weight is the probability of the document being generated by a specific topic in the \gls{lda} model.
The weight for a document comes from the topic probability distribution $\theta$ for the document.
The document chosen to represent a given topic is the document with the highest weight for the topic out of all documents.

%Analysis
By examining the content of the representative document for each topic, some deeper knowledge about what the topics cover can be discovered.
The document for topic 0 appears to be an article about the Islamic State (IS), cities and territories related to them, and terrorism.
Topic 0 includes highly weighted words such as "land" (country), "vejr" and "vind" (weather and wind), "Nordkorea" (North Korea), and "is", which might be Islamic State.
These observations seem to indicate that this topic is about countries and news related to this.
The document for topic 1 is about a municipality in Denmark giving 5 million kroner to help build a national park.
Some highly weighted words for this topic are "krone" (Danish currency), "virksomhed" (company), "penge" (money), and "arbejde" (work).
This topic seems to be about money in general, as well as corporations and work, which is not seen in the document.
The document for topic 2 is the least informative of the documents so far.
It is a general article about what events are happening in the near future, with no specific topic.
Since the highly weighted words in the topic also do not seem to have a connection, it might be a topic specifically made of this type of article, which would make the topic not very useful in practice.
It might be worth removing these kinds of articles in the preprocessing, since this could generate a clearer topic.
The document for topic 3 is about the White House appointing a new communications director.
This fits well with the highly weighted words from the topic, such as "hus" (house), "præsident" (president), and "amerikansk" (American), and indicates that it is a topic about the American presidency, and the White House in general.
Finally, the document for topic 4 is a review of different wines.
This shows a clear correlation to the highly weighted words of the topic, such as "god" (good), "pris" (price), "år" (year), and "vin" (wine).
This indicates that topic 4 is mostly about wines and articles related to this.

These observations show that the topics and documents seem to be correctly correlated.
Though there will still appear topics that are not clearly understandably for humans, such as topic 2.

Another observation worth noting is that it is clear by looking at the weights of the documents, that the longer the document is, the higher the probability of being generated by a specific topic is.
This makes sense since the more information the \gls{lda} model has to work with, the more precisely it can place a document in a topic.

%Hvis vi føler tabellen er for stor, kan vi f.eks. ændre det til kun at vise de første 50-100 ord fra hvert dokument
\input{tables/topic_documents.tex}
