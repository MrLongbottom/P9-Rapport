\subsection{Encountered Problems}
\todo[inline]{indicate whether the problems were solved.}

\subsubsection{Adjacency Matrix Construction}
Our adjacency matrix is based on whether documents share some topics in their topic distributions. 
This leads us to the problem of $D*D*T$ insertion operations into a sparse matrix with what was originally ~50.000 documents. We never tested how long this would end up taking, but it was definitely infeasible.
We did a lot of preprocessing, which reduced this number eventually all the way down to ~30.000 documents, but it was too insignificant to solve anything. Calculation time was now estimated to be ~140 days.
However our similarity function is symmetric, so we cut the time in half by only constructing half of the adjacency matrix. Calculation time was now estimated to be ~69 days.
We then changed our algorithm to only consider documents that shared topics to the original documents, decreasing our time to $D*T_d*D_t$, where both of the new variables are reduced compared to the originals. This reduced the calculation time to be estimated as ~48 days.
We also made the calculation multithreaded in order to reduce computation time. Estimated calculation time is reduced to ~2,4 days using 8 threads.
Lastly, we changed to construct our matrix using the \emph{lil\_matrix} format, which has faster insertions. This last change made the construction much more manageable.


\todo[inline]{Nedenstående problem skal uddybes, hvis det beholdes. Skal også have sit eget afsnit separat fra Adjacency Matrix Construction}
The function get\_document\_topics might be needing the whole corpus since whenever we give it the query it returns the same two topics. 
These two topics are probably the two most general topics within the model which might spell trouble since we don't know how to remove these from the model.

\subsubsection{Dirichlet distributions only produce non-zero values}
We use document-topic and topic-word distributions produced by the Dirichlet distributions to construct our adjacency matrix and to convert queries into topic distributions. 
However as part of how \gls{lda} is designed it has no zeros in its $\theta$ and $\beta$ values, only extremely low numbers.
This is not optimal for constructing an adjacency matrix, as it would result in a fully connected graph.
Adjusting $\alpha$ and $\eta$ will make the low values lower, and the high values higher, but this does not make the problem disappear.
We have therefore introduced thresholds.
These thresholds reduce all values lower than the threshold to zero in the document-topic matrix and topic-word matrix.
The new problem introduced by introducing thresholds is that the exact values of the thresholds are now added as extra hyper-parameters for our solution.

\subsubsection{Unable to reverse engineer the \gls{lda} generative process}
In the generative process of \gls{lda}, a document $d$ is assumed to have a certain underlying topic-distribution $\theta_d$ and a topic $k$ is assumed to have a certain underlying word-distribution $\beta_k$.
In \gls{lda}, words are generated for documents by choosing a topic from the topic-distribution of the document and then choosing a word based on the word-distribution of that topic.

Our initial idea for query handling was to reverse engineer this process.
For each word in a query, we would convert a word into a topic-distribution and then convert that topic-distribution into a document distribution.
This would be done using the same tables that were used to create the words $\theta$ and $\varphi$, but by accessing columns rather than rows, which also meant that the array of numbers we received had to be converted into distributions, as only the rows of these matrices were guaranteed to sum to 1.

To test this initial query handling we generated some queries by taking the 4 highest-ranking words in the document using tf-idf. Using this query we would then see if our algorithm could find the document again only using the words in the query.

We hoped that each word would only be a significant part of a few topics and that each topic was only a significant part of a few documents.
If the topic-distribution only contained a few high values, one or more of which would be among the topics belonging to the original document, then we might be able to find the original document, based on the query.

This reverse engineering process formed the base of our query handling, where we converted a query of words into a personalization vector (containing a value between 0 and 1 for each document).

However, when we tested this process, we found that the distributions were far too evenly valued to get much meaning out of them.
To remedy this, we made the distributions contain fewer values by converting all values under a given threshold to 0 and then normalizing the values.
While this resulted in more decisive distributions, the results were very bad, often ranking the original document very low.
Furthermore, under half the words from the query would have even a single matching topic with the original document, despite usually having more topics in their distribution than the document's distribution.
 
In the end we decided that reverse engineering the \gls{lda} process, was not an effective strategy, and went on to try getting more context to our queries using 'query extension'.


\subsubsection{Duplicates within the dataset}
Within Nordjyske's data, each article is published in several different magazines. 
This results in the dataset consisting of many duplicates. 
To combat this problem, we make a dictionary of articles, so that we only process unique articles.
However when we have preprocessed the Nordjyske data, there are still some duplicates, since articles containing information like football scoreboards, contain different scores before preprocessing but the same information after preprocessing, since we remove numbers from the articles.


\subsubsection{Problems with word value multiplication}
We use \autoref{eq:query_prob} to calculate the probability of a query having been generated by a specific document. 
However, because all word probabilities $P(w|d)$ are multiplied together, having a value of 0 for just one word will make the whole query have a value of 0.
This has shown to be a problem in several of our baselines (namely \gls{lm} and \gls{tf-idf}), often gives words a value of 0.
One of the consequences of this problem is that the algorithms give the same value for all queries containing at least one 0-value word, even when the other words have different values. 
Queries that contain one 0-value word are also evaluated the same as queries containing multiple 0-value words.

To counteract this problem, we have implemented a minimum value when calculating this equation. 
Each 0-value is replaced with $?$\todo{insert value}. 
This value is so low that the query will always score lower than queries containing no 0-value words, but it makes it possible to sort queries containing one or more 0-value words.
