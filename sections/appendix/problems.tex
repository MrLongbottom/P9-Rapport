\subsection{Encountered Problems}

\subsubsection{Too many low values}
The document-topic distributions seem to have too low entropy and a lot of very low values.
This causes each topic to have an average of $\sim$7000 docs in their distribution, however with very low entropy, meaning that some of these distributions are very insignificant, but still add space and time complexity to our sparse representations.
To fix this we try tuning a minimum threshold for a topic to be counted in the document-topic distributions, which can be set as a hyper-parameter for \emph{gensim.ldamodel.get\_document\_topics()}.
After setting up a minimum percent of 2.5\% for an entry to be included in the document distribution, the number of documents per topic fell to ~4800, removing more than 5 entries per document for a total of $\sim$350.000 entries deleted. 
This has overall made the matrices a lot more sparse, but we will still have to evaluate whether changing K is better.

\subsubsection{Empty Topics}
There are also a significant number of topics, which had words, but no assigned docs. 
These should be detected and removed, though their existence points towards a lower $K$ might be needed.

\subsubsection{Too Common Topics}
On the opposite end, we also have some topics which are so universal that almost all documents are included in this topic. 
This might be a sign that pre-processing is not working well enough, or it might once again point at the needed for a better K.
Alternatively, too general topics could be manually pruned after the LDA model has been run.

\subsubsection{Adjacency Matrix Construction}
Our adjacency matrix is based on whether documents share some topics in their topic distributions. 
This leads us to the problem of D\*D\*T insertion operations into a sparse matrix with what was originally ~50.000 documents. (unknown)
We did a lot of prepossessing, which reduced this number eventually all the way down to ~30.000 documents, but it was too insignificant to solve anything. (~140 days)
However our similarity function is symmetric, so we cut the time in half by only constructing half of the adjacency matrix. (~69 days)
We then changed our algorithm to only consider documents that shared topics to the original documents, decreasing our time to D\*T\_d\*D\_t, where both of the new variables are reduced compared to the originals. (~48 days)
We also made the calculation multithreaded in order to reduce compution time (~2,4 days)
Lastly, we changed to construct our matrix using the \emph{lil\_matrix} format, which has faster insertions.


The function get\_document\_topics might be needing the whole corpus since whenever we give it the query it returns the same two topics. 
These two topics are probably the two most general topics within the model which might spell trouble since we don't know how to remove these from the model.

\subsubsection{Dirichlets produce Non-Zero values}
We use distributions produced by the Dirichlet's to construct our adjacency matrix and to convert queries into topic distributions. 
However as part of how LDA is designed it never has 0 values in its distribution values, only extremely low numbers.
This is not optimal for constructing an adjacency matrix, as it would result in a fully connected graph.
Adjusting $\alpha$ and $\eta$ will make the low values lower, and the high values higher, but the problem never disappears from this.
We have therefore introduced thresholds. 
These thresholds reduce all values lower than the threshold to zero in the document-topic matrix and topic-word matrix.
The new problem introduced by introducing thresholds is that the exact values of the thresholds are now added as extra hyper-parameters for our solution.

\subsubsection{Unable to reverse engineer the \gls{lda} generative process}
In the generative process of \gls{lda}, a document $d$ is assumed to have a certain underlying topic-distribution $\theta_d$ and a topic $k$ is assumed to have a certain underlying word-distribution $\beta_k$.
Words are generated in the document by choosing a topic from the document based on the topic-distribution of the document and then choosing a word based on the word-distribution of that topic.

Our initial idea for query handling was to revert this process.
For each word in a query, we would convert a word into a topic-distribution and then convert that topic-distribution into a document distribution.
This would be done using the same tables that were used to create the words $\theta$ and $\varphi$, but by accessing columns rather than rows, which also meant that the array of numbers we received had to be converted into distributions, as only the rows of these matrices were guaranteed to sum to 1.

To test this initial query handling we generated some queries by taking the 4 highest-ranking words in the document using tf-idf. Using this query we would then see if our algorithm could find the document again only using the words in the query.

We hoped that each word would only be a significant part of a few topics and that each topic was only a significant part of a few documents.
If the topic-distribution only contained a few high values, one or more of which would be among the topics belonging to the original document, then we might be able to find the original document, based on the query.

This reverse engineering process formed the base of our query handling, where we converted a query of words into a personalization vector (containing a value between 0 and 1 for each document).

However, when we tested this process, we found that the distributions were far too evenly distributed between their values to get much meaning out of them.
To remedy this, we made the distributions contain fewer values by converting all values under a given threshold to 0 and then normalizing the values.
While this resulted in more decisive distributions, the results were very bad, often ranking the original document very low.
Furthermore, significantly under half the words from the query would have even a single matching topic with the original document, despite usually having more topics in their distribution than the document's distribution.
 
In the end we decided that reverse-engineering the \gls{lda} process, was not an effective strategy, and went on to try getting more context to our queries using 'query extension'.
