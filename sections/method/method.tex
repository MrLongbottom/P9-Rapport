\section{\gls{ir} Methods}\label{sec:ir_methods}

In this section we will be introducing the \gls{ir} methods that we will be testing.
We have chosen these methods to evaluate different ways of performing a \acrfull{ir} task.

The goal of each method is to produce a document-score vector with one score for each document in the corpus, indicating the relevance of the document compared to the query.
These scores can then be sorted in descending order to produce a ranking of the documents.

Some of the methods calculate scores for a single word at a time rather than a whole query.
In that case, we treat their score is an estimate of the probability a document $d$ generating a specific word $w$.
To find the probability of $d$ generating a given query $q$, we take the product of the word probabilities for each word in the query $w \in q$, as the document would have to generate each word, as shown in \autoref{eq:query_prob}.

\begin{equation}\label{eq:query_prob}
	P(q|d) = \prod_{w \in q} P(w|d)
\end{equation}

\input{sections/method/topic_modelling}
\input{sections/language_model}
\input{sections/method/PageRank.tex}

\subsection{\acrlong{tf-idf}}
\Gls{tf-idf} is a well known \gls{ir} method which is used to find the most important words within text.
As the name suggests, this methods aims to balance the two measures: term frequency and inverse document frequency.
For our case, terms are the words left in the documents after the preprocessing phase.
For each term in the query $t \in q$, a tf-idf score will be calculated for each document in the corpus $d \in D$.
Thus each document will have one score for each term in he query.


Term frequency describes how often a word is used in the specific document that is being evaluated.
Inverse Document Frequency describes how many documents in the corpus includes the given term, with a higher percentage of documents producing a lower value.
The overall goal of \gls{tf-idf}, is giving a measure of how often used and unique a term is for a given document. Words that are often used in the given document, but rarely used in the corpus will have the highest \gls{tf-idf} scores.

The formula for \gls{tf-idf} can be seen in \autoref{eq:tfidf}

\begin{equation}\label{eq:tfidf}
	\text{tf-idf}(t, d, D) = \text{tf}(t, d) \cdot \text{idf}(t, D)
\end{equation}

\begin{equation}\label{eq:idf}
	\text{idf}(t,D) = log \frac{|\{d \in D\}|}{|\{d \in D : t \in d\}|}
\end{equation}
	
Where tf$(t, d)$ is the number of times a term $t$ appears in a document $d$ from the corpus $D$.


\subsection{\acrlong{bm25}}
\gls{bm25} is a bag of words retrieval function, based on \gls{tf-idf}.
However, \gls{bm25} introduces two new concepts that it accounts for: document length and term saturation.

The idea of adjusting term frequency based on document length is not new. 
It comes from the idea that long documents will naturally have higher term frequencies and are therefore unfairly scored higher.
The intuition is to multiply the term frequency by the document length relative to the average document length in the corpus $tf * \frac{dl}{adl}$.
However, not all document sets are necessarily equally affected by document length.
So, \gls{bm25} introduces a hyperparameter $b$ that adjust how sensitive a corpus should be to varying document length.

Term saturation comes from a relaxation of an assumption made by \gls{tf-idf}.
In \gls{tf-idf} having a term frequency of 2000 is considered two times as relevant as a term frequency of 1000.
The intuition is that the more a term is used in a document, the more likely the document is to be about that term.
However in practice, once a term has been used enough, we are pretty sure that the document is about that term, and using it further doesn't make the document linearly more relevant.
\gls{bm25} introduces a second hyperparameter $k$ that adjusts how quickly a term is saturated based on the form $\frac{tf}{tf+k}$.

The weaknesses of \gls{bm25} come from it introducing more hyperparameters that will have to be adjusted based on the corpus.
If these hyperparameters are not tuned properly it could achieve bad performance.
However, with good tuning of the parameters \gls{bm25} usually outperforms \gls{tf-idf}.

The full formula for \gls{bm25} can be seen in \autoref{eq:bm25}\cite{bm25}.
\begin{equation}\label{eq:bm25}
	\text{bm25}(d, q) = \sum_{i=1}^{n}\text{idf}(q_i) \cdot \frac{\text{tf}(q_i, d) \cdot (k_1 + 1)}{\text{tf}(q_i, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{avgdl})}
\end{equation}
where tf$(q_i)$ is the number of times the term $q_i$ appears in $d$.
$|d|$ is the number of words in d. 
$avgdl$ is the average document length from the corpus.
$b$ and $k_1$ are both hyperparameters, which for our implementation are set to $0.75$ and $1.5$, respectively.
We used the python package \texttt{rank-bm25}\footnote{\url{https://pypi.org/project/rank-bm25/}} for the implementation of \gls{bm25}.


\subsection{Combining \gls{ir} Methods}
Each of the \gls{ir} methods described in this section has their own strengths and weaknesses.
We combine multiple of these methods, to see if they are able to draw upon each others strengths and cover each others weaknesses.
We do with the hope of a combination of methods being able to produce better results than each of the methods separately.

As with \citet{yang2009topic}, we combine multiple \gls{ir} methods together, by normalizing their document-score vectors and then either summing or multiplying them together element-wise.
This process is visualized in \ref{fig:combine}.

\input{figures/overview_figure.tex}

Both of these two options are viable, and have their own benefits and drawbacks.
Summation allows documents to be ranked fairly high even if one of the combined methods produce a low score.
With multiplication a document with average scores for each method can have a better rank than one with mostly good scores but a really low score from one of the combined \gls{ir} methods.
