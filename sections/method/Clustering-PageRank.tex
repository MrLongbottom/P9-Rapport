\section{Cluster-based Random Walk}\label{sec:cluster_pagerank}
To perform random walk on a document set, such as Nordjyske, probabilities of jumping between documents is needed to perform each step. 
When performing a random walk, we get the articles that most important based on the information that the probabilities are based on. 
If it was based on word frequency, a random walk would return an ordered list of articles, where the ordering is based on the frequency of a given word.
 
We have chosen to use a similarity measure between each document as these probabilities.
This is calculated by using the document-topic distributions from the \gls{lda}.
These distributions can be arranged in a matrix $\theta$, which is described later.

In \cite{ClusterPageRank}, they describe the \gls{Cluster-CMRW} which is a new version of the random walk model. 
They improve upon this model by incorporating information from clusters. 
The clusters are found by employing three different clustering algorithms.
They create a new transition probability matrix where the cluster information is added by combining three similarity functions based on the information levels present in the data.
They describe three similarity levels between:
\begin{itemize}
    \item Sentence to Sentence - $f(v_i \rightarrow v_j)$
    \item Sentence to Topic Cluster - $\omega(v_i, clus(v_i))$
    \item Topic Cluster to Document Set - $\pi(clus(v_i))$
\end{itemize}
where $v_i$ is the given sentence and $clus(v_i)$ is the cluster that $v_i$ belongs to.

We want to use this method to incorporate the information given by the topic distributions within a \gls{Cluster-CMRW}. 
We first create three similarity functions to incorporate topic cluster level information into a new adjacency matrix.
\begin{itemize}
    \item Document to Document - $f(d_i \rightarrow d_j)$
    \item Document to Topic Cluster - $\omega(d_i,vlus(d_i))$
    \item Topic Cluster to Document Set - $\pi(clus(d_i))$
\end{itemize}

\noindent
Where $d$ is a document, $clus(d_i)$ is a topic cluster of document $d_i$.
Notice that a document $d$ can contain multiple topics, as opposed to normal clustering where each element is generally assumed to only be part of one cluster.

\subsection*{Document to Document similarity}
We define the similarity of two articles as the distance between their distribution, which is calculated using square root of Jensen-Shannon divergence, which is often called Jensen-Shannon distance\cite{jensen-shannon2003}\cite{jensen-shannondis2003}.
This metric gives a score between 0-1 based on the distance between two distributions, where a low distance is equal to more similar.
So our similarity function is:
$$ f(d_i \rightarrow d_j) = 1 - \text{jenson-shannon-distance}(d_i, d_j)$$ 

\subsection*{Document to Topic Cluster similarity}
We define the similarity between a document and a topic cluster as the document-topic probability distribution between them.
$$ \omega(d_i,clus(d_i)) = \theta_{d_i,clus(d_i)}$$
where $\theta$ is the document-topic distribution matrix.

\subsection*{Topic Cluster to Document Set similarity}
We define the similarity between a topic $t$ and the whole document set as:
$$ \pi(clus(d_i)) = \frac{\sum_{d}^{M} \theta_{clus(d_i)}}{M} $$
where $M$ is the number of documents in the document set.


To create the adjacency matrix, \cite{ClusterPageRank} linearly combine the different similarity functions, in which we intend to do the same.
Formally, the adjacency matrix is calculated with the following function.
$$ f(d_i \rightarrow d_j | clus(d_i), clus(d_j)) $$
which is evaluated to 

\begin{align*}
&f(d_i \rightarrow d_j | clus(d_i), clus(d_j)) = \\
&f(d_i, d_j) \cdot (\lambda \cdot \pi(clus(d_i))) \cdot \omega(d_i, clus(d_i)) \\ 
&+ (1-\lambda) \cdot \pi(clus(d_j)) \cdot \omega(d_j, clus(d_j))
\end{align*}

where $\lambda$ is a weight between $[0,1]$ that controls the relative contribution of the two clusters.