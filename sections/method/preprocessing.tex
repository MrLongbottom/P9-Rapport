\subsection{Preprocessing}\label{subsec:prepro}

Our preprocessing consists of several steps, each of which will be described in more detail.

\begin{enumerate}
	\item Article Extraction
	\item Initial Document filtering
	\item Stopwords and Word Frequency
	\item Word Databases / POS Tagging
	\item Stemming
	\item Repeat step 2 and Stopwords
	\item Construct Files
\end{enumerate}

\paragraph{Article Extraction}
Documents are loaded from \texttt{.xml} files, and their title and body contents extracted and converted into \texttt{.json} format.

\paragraph{Initial Document Filtering}
Duplicate documents are removed. Documents containing fewer than 20 words are also removed.

\paragraph{Stopwords and Word Frequncy Filtering}
Stopwords are removed, using \emph{nltk.corups.stopwords}. Words are removed if they appear fewer than 25 times in the document set or if they used in over 25\% of the documents.

\paragraph{Words Database Filtering / POS-Tagging}
Words are checked against the two word databases: Wiktionary and DanNet. The databases contain only: nouns, adjectives, verbs, adverbs, and proper nouns. Words that are not contained within either database are removed.

\paragraph{Stemming}
Words are reduced to their stems using \texttt{nltk.stem.snowball.DanishStemmer}. This is done in order to combine word declensions, which reduced the number of words without removing too much information.
Words which stem are stopwords are also removed at this time.

\paragraph{Construct files}
Construct and save scikit-learn.CountVectorizer as a sparse matrix \texttt{.npz} file, and word2vec, doc2vec, doc2word as \texttt{.csv} files.


\subsection{Post-LDA preprocessing}
After LDA has made a topic model, documents which have no accociated topic, and topics which have none accociated documents would be removed.
Topics which almost every documents are a part of are also considered too general to be useful and would also be removed.
This initially has some implementation challenges, as it would require removing topics and documents from generated files and from the LDA model, in order to be able to use the right topic/document IDs in the future.

\subsection{Query Preprocessing}
One of the challenges of preprocessing queries is that they usually contain very few words, meaning that there is a huge possibility of information loss during preprocessing and conversion, as there is not much content or context to be learned from.

Our na√Øve first method is to convert each word of the query into a topic distribution, making the query have the same format as a document. This enables us to compare the query to other documents, and therefore do a random walk starting from the query.
The danger of this methods is that a query only containing a couple of words, will likely have a very scewed topic distribution.