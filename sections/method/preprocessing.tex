\subsection{Preprocessing}\label{subsec:prepro}

Our preprocessing consists of several steps, each of which will be described in more detail.
The steps are:
\begin{enumerate}
	\item Article Extraction
	\item Initial Document Filtering
	\item Stopwords and Word Frequency
	\item Word Databases / POS Tagging
	\item Stemming
	\item Repeat step 2 and Stopwords
	\item Construct Files
\end{enumerate}

\paragraph{Article Extraction}
Documents are loaded from \texttt{xml} files, and their ID, title, and body contents extracted and converted into \texttt{json} format.

\paragraph{Initial Document Filtering}
Duplicate documents and documents containing fewer than 20 words are removed.

\paragraph{Stopwords and Word Frequency Filtering}
Stopwords are removed, using \emph{nltk.corpus.stopwords}. 
Words are removed if they appear fewer than 25 times in the document set or if they are used in over 25\% of the documents.

\paragraph{Words Database Filtering / POS-Tagging}
Words are checked against the two word databases: Wiktionary and DanNet. 
The databases contain only: nouns, adjectives, verbs, adverbs, and proper nouns. 
Words that are not contained within either database are removed.

\paragraph{Stemming}
Words are reduced to their stems using \texttt{nltk.stem.snowball.DanishStemmer}. 
This is done to combine word declensions, which reduces the number of words without removing too much information.
Words that are stemmed into stopwords are also removed at this time.

\paragraph{Construct Files}
Construct and save scikit-learn.CountVectorizer as a sparse matrix \texttt{npz} file, and word2vec, doc2vec, doc2word as \texttt{csv} files.


\subsection{Post-LDA Preprocessing}
After LDA has made a topic model, documents which have no associated topics, and topics which have no associated documents would be removed.
Topics that occur in almost every document are also considered too general, and would also be removed. \todo[inline]{Tjek senere at Post-LDA preprocesseringen bliver brugt i pipelinen.}
Initially, this presented some implementation challenges, as it required removing topics and documents from files generated earlier in the process and from the LDA model, to be able to use the right topic/document IDs in the future.

\subsection{Query Preprocessing}
One of the challenges of preprocessing queries is that they usually contain very few words.
Therefore, there is a huge possibility of information loss during preprocessing and conversion, as there is not much content or context to be learned from.

Our method converts each word in the query into a topic distribution, making the query have the same format as a document. \todo[inline]{Detaljer specifikt hvordan denne distribution laves, når det er færdigimplementeret.}
This enables us to compare the query to other documents, and therefore do a random walk based on the query.
The danger of this method is that a query only containing a couple of words, will likely have a very skewed topic distribution.
