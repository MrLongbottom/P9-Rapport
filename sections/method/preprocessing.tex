\section{Preprocessing}\label{sec:prepro}

In this section we will give a detailed overview over our method of preprocessing documents.
Our preprocessing consists of several steps, each of which will be described in more detail, along with a running example over part of a real document.
\begin{enumerate}[label=\alph*]
	\item Article Extraction
	\item Initial Document Filtering
	\item Stopword and Word Frequency Filtering
	\item Word Databases / POS Tagging
	\item Stemming
	\item Repeat Document Filtering
	\item Repeat Stopword Filtering
	\item Construct Files
\end{enumerate}

\paragraph{Article Extraction}
As decribed in \autoref{sec:dataset}, documents are loaded as \texttt{xml} files from the database, and their ID, title, and body contents extracted and converted into \texttt{json} format.
\todo[inline]{make running example}

\paragraph{Initial Document Filtering}
The database contain many duplicate documents with the same content but different file names, we therefore start by removing all duplicates.
We also remove documents containing fewer than 25 words. These documents are assumed to contain too little information to be usefull. This also has the added benefit of removing ads, or similar small sections that are present in newspapers, but aren't really articles.
\todo[inline]{make running example}

\paragraph{Stopword and Word Frequency Filtering}
Stopwords are removed, using \emph{nltk.corpus.stopwords}.
Words are removed if they appear fewer than 25 times in the document set or if they are used in over 25\% of the documents.
\todo{find article to ref to constants}
\todo[inline]{make running example}

\paragraph{Words Database Filtering / POS-Tagging}
Words are checked against the two word databases: Wiktionary\footnote{https://github.com/fnielsen/awesome-danish (specifically Danish resources: https://people.compute.dtu.dk/faan/ps/Nielsen2016Danish.pdf)} and DanNet\cite{Pedersen2009DanNetTC}. 
The databases contain only: nouns, adjectives, verbs, adverbs, and proper nouns. 
Words that are not contained within either database are removed.
\todo[inline]{make running example}

\paragraph{Stemming}
Words are reduced to their stems using \texttt{nltk.stem.snowball.DanishStemmer}. 
This is done to combine word declensions, which reduces the number of words without removing too much information.
\todo[inline]{make running example}

\paragraph{Repeat Document Filtering}
After having removed words we once again remove documents that do not meet the minimum requirement of 25 words.

\paragraph{Repeat Stopword Filtering}
After stemming has been applied some words might have been converted into stopwords.
We therefore remove stopwords once again.


\paragraph{Construct Files}
Construct and save scikit-learn.CountVectorizer as a sparse matrix \texttt{npz} file, and word2vec, doc2vec, doc2word as \texttt{csv} files.\vejleder[inline]{what is the structure of the csv files and why do we create them?}
\todo[inline]{make running example}

\subsection{Post-LDA Preprocessing}
After LDA has made a topic model, documents which have no associated topics, and topics which have no associated documents would be removed.
Topics that occur in almost every document are also considered too general, and would also be removed. \todo[inline]{Tjek senere at Post-LDA preprocesseringen bliver brugt i pipelinen.}
Initially, this presented some implementation challenges, as it required removing topics and documents from files generated earlier in the process and from the LDA model, to be able to use the right topic/document IDs in the future.
\todo[inline]{make running example}

\subsection{Query Preprocessing}
One of the challenges of preprocessing queries is that they usually contain very few words.
Therefore, there is a huge possibility of information loss during preprocessing and conversion, as there is not much content or context to be learned from.

Our method converts each word in the query into a topic distribution, making the query have the same format as a document. \todo[inline]{Detaljer specifikt hvordan denne distribution laves, når det er færdigimplementeret.}
This enables us to compare the query to other documents, and therefore do a random walk based on the query \vejleder[inline]{say why this is usefull}.
The danger of this method is that a query only containing a couple of words, will likely have a very skewed topic distribution.
\todo[inline]{make running example}
