\section{Preprocessing}\label{sec:prepro}

In this section, we will give a detailed overview of our preprocessing methods.
Our preprocessing steps were inspired by \cite{quanti}, who do a thorough preprocessing before applying \gls{lda} to a dataset of news articles. 
Our preprocessing consists of several steps, each of which will be described in more detail, along with a running example over part of a real document.
The preprocessing steps are:
\begin{enumerate}[label=\alph*)]
	\item Article Extraction
	\item Initial Document Filtering
	\item Stopword and Word Frequency Filtering
	\item Word Database Filtering
	\item Lemmatization 
\end{enumerate}

\begin{table}[b]
	\caption{Number of documents and unique words remaining after each preprocessing step.}
	\begin{tabular}{c|c|c}
		Preprocessing Step & Documents & Words\\
		\hline
		Article Extraction & 63.266 & 391.977 \\ 
		Initial Document Filtering & 32.201 & 190.133 \\ 
		Stopword and Word Frequency Filtering & 32.201 & 16.406 \\
		Word Database Filtering & 32.201 & 14.537 \\
		Lemmatization & 32.201 & 9.934\\
	\end{tabular}
	\label{tab:prepro_doc_word}
\end{table}

The number of documents and unique words remaining after each preprocessing step can be seen in \autoref{tab:prepro_doc_word}.

\paragraph{Article Extraction}
As described in \autoref{sec:dataset}, documents are loaded as \texttt{xml} files from the database, and their ID, title, and body contents are extracted and converted into \texttt{json} format.
\autoref{prepro:example1} shows an example of an extracted document.

\paragraph{Initial Document Filtering}
The dataset contains many duplicate documents with the same content but different file IDs.
Therefore, we start this phase by removing all duplicates.
During this step, all punctuation, numbers, and other non-alphabetic symbols are also removed, keeping only words entirely made of alphabetic symbols.

Afterward, we remove documents containing fewer than 25 words.
This is done to remove advertisements, sports results, snippets, or other similar small sections that are present in newspapers but are not really news articles.
\autoref{prepro:example2} shows an example of a document that would be removed. In this example, the document is merely a front-page snippet referencing an actual article, rather than a full news article.
\begin{figure}[h]
	\begin{framed}
		\begin{quote}
			\textit{
				'Legedag for børnene Frivillighuset i Pandrup holdt for femte gang Legedag, så alle børn kan fortælle om en god ferieoplevelse. Side 12-13'
			}
		\end{quote}
	\end{framed}
	\caption{Example of a document that was filtered for being too short.}
	\label{prepro:example2}
\end{figure}

%\begin{quote}
%	\textit{
%		'tricktyve fik fingre i dankort hobro frække tricktyve har atter slået til i en af hobros dagligvareforretninger denne gang gik det ud over en årig mand som under indkøbsturen i superbrugsen i adelgade blev frastjålet sit visa dankort med tilhørende pin kode% inden kortet blev spærret nåede tyvene at hæve kr i en jutlander bank automat i hobro og yderligere et beløb i en billetautomat på århus kanten'
%	}
%\end{quote}

\paragraph{Stopword and Word Frequency Filtering}
Stopwords are removed, using \emph{nltk.corpus.stopwords}. 
This is a list consisting of 94 stopwords, like ``og'' and ``på''.
We also added some stopwords ourselves based on manual inspection of words that appeared in almost every topic and had a high topic probability for multiple topics.
We also remove words based on their frequency, where they are removed if they appear fewer than 25 times in the corpus.
These words are considered too rare to be relevant for a topic, as few documents will share these words.
Words are also removed if they appear in over 25\% of the documents.
These words are considered too common to be bound to specific topics and are therefore practically stop words.
These constants were the same used by \cite{quanti}.
\autoref{prepro:example3} shows how the document from \autoref{prepro:example1} has changed after this step.
\begin{figure}[h]
	\begin{framed}
		\begin{quote}
			\textit{
				'fik fingre dankort hobro atter slået hobros gang gik årig mand superbrugsen adelgade dankort tilhørende kode'
			}
		\end{quote}
	\end{framed}
	\caption{Document from \autoref{prepro:example1} after removing stopwords and applying word frequency filtering.}
	\label{prepro:example3}
\end{figure}


\paragraph{Word Database Filtering}
Due to a lack of danish POS-tagging resources, we instead check up the words in our corpus against two databases containing only words of specific word classes, namely: nouns, adjectives, verbs, adverbs, and proper nouns.
In practice, this has the same effect as POS-tagging.
However, words are not marked with their words class, instead they are removed if they do not appear in at least one of the databases.
The two databases used are:  Wiktionary\footnote{\url{https://github.com/fnielsen/awesome-danish} (specifically Danish resources: \url{https://people.compute.dtu.dk/faan/ps/Nielsen2016Danish.pdf})} and DanNet\cite{Pedersen2009DanNetTC}. 

Most words remaining at this step are contained within the databases, so this step does not remove as many words as the other steps.
However, this is mostly due to the step being taken late in the preprocessing process, due to having a longer computation time per word.
This step mostly serves to remove names, and words from different languages, namely English.
However, it also cuts the odd non-words (e.g. 'gimi') and abbreviations (e.g. 'DK') that still exist at this point in the preprocessing.

The example used in \autoref{prepro:example3} does not change at this step. 
Instead, \autoref{prepro:example4} shows an example of another document that does change during this step.

\begin{figure}[h]
	\begin{framed}
		\begin{quote}
			\textit{
				'\colorbox{pink}{country} kræver sommerens by \colorbox{pink}{night} sluttede afslappet band'
			}
		\end{quote}
	\end{framed}
	\caption{Snippet of a document which had words removed during the word database filtering. The highlighted words are removed, due to being foreign words.}
	\label{prepro:example4}
\end{figure}

\paragraph{Lemmatization}
Words are reduced to their lemmas using the python package \texttt{lemmy}\footnote{\url{https://pypi.org/project/lemmy/}}. 
This is done by reducing word inflections.
However, since we do not have proper POS-tagging, some words have multiple meanings depending on the word class.
In these cases we keep all inflected words.
After lemmatization has been applied some words might have been converted into stopwords.
We, therefore, remove stopwords once again.
We also remove any potential documents that may have become empty after all the word filtering, however, this is usually not the case.
The example from \autoref{prepro:example3} after lemmatization can be seen in \autoref{prepro:example5}.
\begin{figure}[h]
	\begin{framed}
		\begin{quote}
			\textit{
				'fingre dankort hobro atter årig adelgade dankort kode'
			}
		\end{quote}
	\end{framed}
	\caption{Lemmatized version of the example document from \autoref{prepro:example3}.}
	\label{prepro:example5}
\end{figure}


%\subsection{Post-LDA Preprocessing}
%After LDA has made a topic model, documents which have no associated topics, and topics which have no associated documents would be removed.
%Topics that occur in almost every document are also considered too general, and would also be removed.
%Initially, this presented some implementation challenges, as it required removing topics and documents from files generated earlier in the process and from the LDA model, to be able to use the right topic/document IDs in the future.
