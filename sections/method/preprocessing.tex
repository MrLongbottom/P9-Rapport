\section{Preprocessing}\label{sec:prepro}

In this section, we will give a detailed overview of our method of preprocessing documents.
Our preprocessing steps were inspired by \cite{quanti}, who do a thorough preprocessing before applying LDA. 
Our preprocessing consists of several steps, each of which will be described in more detail, along with a running example over part of a real document.
The preprocessing steps are:
\begin{enumerate}[label=\alph*]
	\item Article Extraction
	\item Initial Document Filtering
	\item Stopword and Word Frequency Filtering
	\item Word Databases / POS Tagging
	\item Stemming
	\item Construct Files
\end{enumerate}

\begin{table}
	\begin{tabular}{c|c|c}
		Preprocessing Step & Documents & Words\\
		Article Extraction & 63266 & unknown \\ 
		Initial Document Filtering & 32201 & 190133 \\ 
		Stopword and Word Frequency Filtering & 32201 & 16406 \\
		Word Databases / POS Tagging & 32201 & 14537 \\
		Stemming / Lemmatization & 32201 & 8691 / 9934\\
	\end{tabular}
	\label{tab:prepro_doc_word}
	\caption{Amount of documents and unique words remaining after each preprocessing step.}
\end{table}

\paragraph{Article Extraction}
As described in \autoref{sec:dataset}, documents are loaded as \texttt{xml} files from the database, and their ID, title, and body contents extracted and converted into \texttt{json} format.
\autoref{prepro:example1} shows an example of an extracted document.
\begin{figure}[h]
	\begin{quote}
		\textit{
			'Tricktyve fik fingre i dankort HOBRO: Frække tricktyve har atter slået til i en af Hobros dagligvareforretninger. Denne gang gik det ud over en 87-årig mand, som under indkøbsturen i Superbrugsen i Adelgade blev frastjålet sit Visa-Dankort med tilhørende pin-kode.'
		}
	\end{quote}
	\caption{snippet of raw document}
	\label{prepro:example1}
\end{figure}

\paragraph{Initial Document Filtering}
The database contain many duplicate documents with the same content but different file names, we therefore start this phase by removing all duplicates.
All punctuation, numbers, and other non-alphabetic symbols are removed, keeping only words entirely made of alphabetic symbols.

Afterward, we remove documents containing fewer than 25 words.
This is done to remove ads, sports results, snippets, or other similar small sections that are present in newspapers but aren't really news articles.
\autoref{prepro:example2} shows an example of a document that would be removed. In this example, the document is merely a front-page snippet referencing an actual article, rather than a full news article.
\begin{figure}[h]
	\begin{quote}
		\textit{
			'Legedag for børnene Frivillighuset i Pandrup holdt for femte gang Legedag, så alle børn kan fortælle om en god ferieoplevelse. Side 12-13'
		}
	\end{quote}
	\caption{snippet of raw document}
	\label{prepro:example2}
\end{figure}

%\begin{quote}
%	\textit{
%		'tricktyve fik fingre i dankort hobro frække tricktyve har atter slået til i en af hobros dagligvareforretninger denne gang gik det ud over en årig mand som under indkøbsturen i superbrugsen i adelgade blev frastjålet sit visa dankort med tilhørende pin kode% inden kortet blev spærret nåede tyvene at hæve kr i en jutlander bank automat i hobro og yderligere et beløb i en billetautomat på århus kanten'
%	}
%\end{quote}

\paragraph{Stopword and Word Frequency Filtering}
Stopwords are removed, using \emph{nltk.corpus.stopwords}. We also added some stopwords ourselves based on manual inspection of words that appeared in close to every topic and had a high topic probability for multiple topics.
We also remove words based on their frequency. 
Words are removed if they appear fewer than 25 times.
These words are considered too rare to be relevant for a topic, as few documents will share these words.
Words are also removed if they appear in over 25\% of the documents.
These words are considered too common to be bound to specific topics and are therefore practically stop words.
These constants were the same used by \cite{quanti}.
\autoref{prepro:example3} shows how the document from \autoref{prepro:example1} has changed after this step.
\begin{figure}[h]
	\begin{quote}
		\textit{
			'fik fingre dankort hobro atter slået hobros gang gik årig mand superbrugsen adelgade dankort tilhørende kode'
		}
	\end{quote}
	\caption{snippet of raw document}
	\label{prepro:example3}
\end{figure}


\paragraph{Words Database Filtering / POS-Tagging}
Due to a lack of danish POS-tagging resources, we instead check up the words in our document set against two databases containing only words of specific word classes, namely: nouns, adjectives, verbs, adverbs, and proper nouns.
In practice this has the same effect as POS-tagging however words are not marked with their words class, instead they are removed if they do not appear in at least of the databases (containing only words of approved word classes).
The two databases used are:  Wiktionary\footnote{https://github.com/fnielsen/awesome-danish (specifically Danish resources: https://people.compute.dtu.dk/faan/ps/Nielsen2016Danish.pdf)} and DanNet\cite{Pedersen2009DanNetTC}. 

Most words remaining at this step are contained within the databases, so this step does not remove as many words, however, this is mostly due to the step being taken late, due to having a longer computation time per word.
This step mostly serves to remove names, and words from different languages, namely English.
However, does also cut the odd non-word (eg. 'gimi') and abbreviations (eg. 'DK') that still exists at this point in the preprocessing.
The example used in \autoref{prepro:example3} does not change at this step. 
Instead, \autoref{prepro:example4} shows an example of another document that does change during this step.

\begin{figure}[h]
	\begin{quote}
		\textit{
			'\colorbox{pink}{country} kræver sommerens by \colorbox{pink}{night} sluttede afslappet band'
		}
	\end{quote}
	\caption{snippet of raw document}
	\label{prepro:example4}
\end{figure}

\paragraph{Stemming / Lemmatization}
Words are reduced to their stems using \texttt{nltk.stem.snowball.DanishStemmer}. 
This is done to combine word declensions, which reduces the number of words without removing too much information.
After stemming has been applied some words might have been converted into stopwords. 
We, therefore, remove stopwords once again.
We also remove any potential documents that may have become empty after all the word filtering, however, this is usually not the case.
\begin{figure}[h]
	\begin{quote}
		\textit{
			'fingre dankort hobro atter årig adelgade dankort kode inden hæve bank hobro århus få slå hobro gange gang mande mand superbrugs tilhøre'
		}
	\end{quote}
	\caption{LEM}
	\label{prepro:example5}
\end{figure}
\begin{figure}[h]
	\begin{quote}
		\textit{
			'fik dankort hobro slået gang gik årig mand dankort pin kr bank hobro beløb århus fingr att hobro superbrugs adelgad tilhør kod'
		}
	\end{quote}
	\caption{STEM}
	\label{prepro:example6}
\end{figure}

\paragraph{Construct Files}
Construct and save \texttt{CountVectorizer} (word-document frequency matrix) as a sparse matrix \texttt{npz} file. This file is used by the LDA to construct a document-topic matrix and a topic-word matrix.
We also save various lookup files in \texttt{csv} format. \texttt{word2vec}, \texttt{doc2vec} files translate word and document to their ids.
\texttt{doc2word} translates document ids to all the ids of the words used in the document.
These files allow us to only use ids from here on, and then later translate back to words when needed.

%\subsection{Post-LDA Preprocessing}
%After LDA has made a topic model, documents which have no associated topics, and topics which have no associated documents would be removed.
%Topics that occur in almost every document are also considered too general, and would also be removed.
%Initially, this presented some implementation challenges, as it required removing topics and documents from files generated earlier in the process and from the LDA model, to be able to use the right topic/document IDs in the future.

\subsection{Query Preprocessing}
\todo[inline]{move this subsec to query section}
One of the challenges of preprocessing queries is that they usually contain very few words.
Therefore, there is a huge possibility of information loss during preprocessing and conversion, as there is not much content or context to be learned from.

Our method converts each word in the query into a topic distribution, making the query have the same format as a document. \todo[inline]{Detaljer specifikt hvordan denne distribution laves, når det er færdigimplementeret.}
This enables us to compare the query to other documents, and therefore do a random walk based on the query \vejleder[inline]{say why this is usefull}.
The danger of this method is that a query only containing a couple of words, will likely have a very skewed topic distribution.
\todo[inline]{make running example}
