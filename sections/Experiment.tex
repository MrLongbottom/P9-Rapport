\section{Experiment}\label{sec:experiment}

The goal of our experiment is to test two different aspects of quality for combinations of \gls{ir} methods.
We test the \gls{ir} methods' ability to find a specific document based a on query.
To do this, we generate 320 document queries based on documents in the corpus, as described in \autoref{subsec:query_gen_doc}, and then test the \gls{ir} methods' ability to find the original document that each query was based upon.

We also test the \gls{ir} methods' ability understand the queries on a more abstract level and find documents are within the same topic as the query.
To do this, we generate 320 topic queries based on topics from our \gls{lda} model, as described in \autoref{subsec:query_gen_top}, and test the \gls{ir} methods' ability to find all documents that are relevant to the original topic that the query was based upon.

We test combinations of the following \gls{ir} methods (these will be referred to as the baselines):
\begin{itemize}
	\item \Acrlong{lda}-\acrlong{ir} (\acrshort{lda}-\acrshort{ir})
	\item \acrfull{lm}
	\item \acrfull{bm25}
	\item \acrfull{tf-idf}
	\item \acrfull{pr}
\end{itemize}

\gls{lda} and \gls{lda}-\gls{ir} was described in \autoref{sec:lda}.
We use the \gls{lda} implementation from Gensim\footnote{\url{https://radimrehurek.com/gensim/models/ldamodel.html}} which is based on online \gls{lda} from \citet{blei2010online}.
\gls{lm} was described in \autoref{sec:lm} based on the implementation by \citet{yang2009topic}.
\gls{pr} was described in \autoref{sec:pagerank}.


\subsection{\acrlong{tf-idf}}
\Gls{tf-idf} is a well known \gls{ir} method which is used to find the most important words within text.
The formula for \gls{tf-idf} can be seen in \autoref{eq:tfidf}
\begin{equation}\label{eq:tfidf}
	\text{tf-idf}(t, d, D) = \text{tf}(t, d) \cdot \text{idf}(t, D)
\end{equation}
where tf$(t, d)$ is the number of times term $t$ is in document $d$ and idf$(t, D)$ is the inverse document frequency of $t$ in $D$.

\subsection{\acrlong{bm25}}
\gls{bm25} is a bag of words retrieval function, which is similar to \gls{tf-idf}, since it uses the inverse document frequency.
The formula for \gls{bm25} can be seen in \autoref{eq:bm25}\cite{bm25}.
\begin{equation}\label{eq:bm25}
\text{bm25}(d, q) = \sum_{i=1}^{n}\text{idf}(q_i) \cdot \frac{\text{tf}(q_i, d) \cdot (k_1 + 1)}{\text{tf}(q_i, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{avgdl})}
\end{equation}
where tf$(q_i)$ is the number of times the term $q_i$ appears in $d$.
$|d|$ is the number of words in d. 
$avgdl$ is the average document length from the corpus.
$b$ and $k_1$ are both hyperparameters, which are set to $0.75$ and $1.5$, respectively.
 
Each baseline will be given a query and produce a document-value vector with one value for each document indicating the relevance of that document compared to the query.
Our results are based on the ranking order of these values.
When multiple baselines are combined, their document-value vectors are normalized and then added or multiplied together element-wise.

For each baseline, we evaluate the quality of its \gls{ir} for both queries generated based on documents (document queries) and queries generated based on topics (topic queries).
For topic queries, all documents with values higher than the mean of the topic's vector in the document-topic matrix $\theta_t$ are considered ground truth positives.
This also means that while document queries always only have one ground truth positive, topic queries can have several thousand ($\sim$1.000-7.000).
This makes it hard to compare these two results on a single baseline.
However, it is still possible to compare these two results between multiple baselines.

We use \gls{map} to evaluate all baselines\cite{other_map}.
\Gls{map} is calculated as follows\footnote{Equations from \url{https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52} and \url{https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)}}:

\begin{equation}
	MAP = \frac{1}{Q}\sum_{q=1}^{Q} AveP(q)
\end{equation}

\noindent Where $Q$ is the number of queries.

\begin{equation}
	AveP(q) = \frac{\sum_{n = 1}^{D} (precision@n(n) \times rel(n))}{|relevant~documents|}
\end{equation}

\noindent Where $D$ is the number of documents, and $rel(n)$ is 1 if the document at rank $n$ is relevant to the query, otherwise it is 0.

\begin{equation}
	precision@n(n)) = \frac{TP(n)}{TP(n) + FP(n)}
\end{equation}
\noindent Where $TP(n)$ is the number of documents correctly labeled as relevant for the query, in the top $n$ ranked documents, and $FP(n)$ is the number of documents incorrectly labeled as relevant for the query, in the top $n$ ranked documents.

We also use precision@10 and precision@100 for the topic queries.
However since document queries only contain one ground truth positive, $p@n$ doesn't make much sense to calculate. 
Instead, we show the average ranking of the relevant document for each document query.
Each of these metrics are shown as the mean value for all queries generated, described in \autoref{sec:query}.

We hypothesize that \gls{lda}-\gls{ir} can improve the topic \gls{ir} when combined with other baselines and that \gls{pr} is also able to improve the \gls{ir} performance since the adjacency matrix, used by gls{pr}, is based on topic similarities.


\subsection{Hyperparameters}\label{subsec:hyperparameters}
Before setting up our experiment, we run a grid search to find acceptable hyperparameter values for our \gls{lda} model.
% runtime parameters
First, we adjusted parameters that had to do with how the algorithms run to ensure that the model converged, without spending unnecessary resources.

\texttt{Passes} adjusts how many times the model passes over the whole corpus, and can be seen as an 'epoch'. 
This is necessary for smaller datasets to ensure convergence before the algorithm finishes, but for large enough datasets this parameter can have a value of 1, as is the case for \citeauthor{blei2010online}\cite{blei2010online} where their corpus includes 3.3M documents, whereas our dataset is $\sim32000$ documents.

\texttt{Iterations} adjusts the maximum number of iterations of the E-step (from \citeauthor{blei2010online}\cite{blei2010online}, Algorithm 2) that are allowed without all documents having converged. 
Setting this to a high value ensures convergence, but also increases training time. 
In practice, \texttt{iterations} should be set to the lowest possible values where nearly all (> 99\%) documents have converged by the end of the training.
From testing we found 20 \texttt{passes} and 100 \texttt{iterations} was enough to converge 27678 out of 27919 documents (99.14\%).

% model parameters
Once we had assured convergence of our model, we tested hyperparameters for our \gls{lda} model.
These include:
\begin{itemize}
	\item K - the number of topics
	\item $\alpha$ - dirichlet prior for document-topic distributions
	\item $\eta$ - dirichlet prior for topic-word distributions
\end{itemize}
Similar to clustering algorithms, there is usually an optimal setting for K, where all values above will produce topics that are subtopics of the optimal topics, and all values below will achieve subpar performance.
This theoretical optimal value is however based on the corpus, so tests are needed to find some setting of K that is close to the optimal.
$\alpha$ and $\eta$ adjusts the Dirichlet distributions, which will create the multinomial distributions for document-topic and topic-word relations, respectively.
Lower values will lead to more uneven distributions, favoring fewer topics per document and fewer words per topic, while higher values will make the distribution more uniform.

To find the optimal hyperparameter values, we run a hierarchical grid-search consisting of two sequential grid-searches using the hyperparameter values shown in \autoref{tab:params}.
The first grid-search used the values for $K_1$, while the second used the $K_2$ values.
In both grid-searches the same $\alpha$ and $\eta$ values are used.
To evaluate the \gls{lda} models, we measured perplexity which describes how well a model can predict new test samples. 
During the first grid-search, we found that $10$ and $50$ were clearly the best values for K.
However, $\alpha$ and $\eta$ values showed no clear favorite.
For the second grid-search we choose more specific K values, to narrow the search, while keeping the $\alpha$ and $\eta$ values the same.
The manual evaluation consisted of checking each topic, within each model and validating whether the topic had any meaningful word correlations like "football" and "coach".


Based on the perplexity and manual evaluation of topics generated by the second grid-search we choose $K = 30$, $\alpha = 0.1$, and $\eta = 0.1$ as the hyperparameters for the final \gls{lda} model.
A new model is generated using these values and this model will be used for the whole experiment.

\begin{table}[h]
	\centering
	\begin{tabular}{c|c}
		Parameter & Tested Values\\
		\hline
		$K_1$ & 10, 50, 100, 200, 300\\
		$K_2$ & 5, 10, 15, 20, 25, 30, 35, 40, 45, 50\\
		$\alpha$ & 0.5, 0.1, 0.01, 0.001\\
		$\eta$ & 0.1, 0.01, 0.001, 0.0001\\
	\end{tabular}
	\caption{Tested hyperparameter values. $K_1$ is the $K$ values used for the first hyperparameter test, while $K_2$ is the $K$ values used for the second hyperparameter test.}
	\label{tab:params}
\end{table}


\subsection{Results}\label{subsec:results}

Results are shown on \autoref{tab:results} showing the \acrlong{map} of each model, \autoref{tab:results_precision_at_10} showing the precision@10 and precision@100 of each model for topic queries, and \autoref{tab:hit_results} showing the average rank of each model for document queries.
The number for the query sets represent the length of the queries.

\autoref{tab:results} and \autoref{tab:hit_results} show that \gls{bm25} is clearly the best model for document query retrieval, with all the best results using \gls{bm25}, sometimes combined with \gls{pr} and/or \gls{lda}-\gls{ir}. 
However \gls{bm25} does not fare well with topic query retrieval, as shown in \autoref{tab:results} and \autoref{tab:results_precision_at_10}.

Combinations of \gls{tf-idf} are omitted from the results as the initial results of \gls{tf-idf} were worse than \gls{bm25} and the two algorithms serve the same purpose for our experiment.


\begin{table*}[h]
	\centering
	\caption{\acrlong{map} results for the four sets of document queries and topic queries.}
	\begin{tabular}{l|c|c|c|c|c|c|c|c}
		& \multicolumn{4}{c|}{Document Query Length} & \multicolumn{4}{c}{Topic Query Length} \\
		Model / \gls{map} & 1 & 2 & 3 & 4 & 1 & 2 & 3 & 4 \\
		\midrule
		\gls{lda}-\gls{ir} & 0.00457 & 0.00527 & 0.0429 & 0.0538 & 0.155 & 0.186 & 0.168 & 0.178 \\
		\gls{lm} & 0.198 & 0.152 & 0.291 & 0.260 & 0.126 & 0.130 & 0.128 & 0.129 \\
		\gls{bm25} & 0.270 & 0.656 & 0.866 & \textbf{0.908} & 0.155 & 0.158 & 0.155 & 0.161 \\
		\gls{tf-idf} & 0.210 & 0.621 & 0.799 & 0.897 & 0.155 & 0.157 & 0.155 & 0.161 \\
		\gls{lda}-\gls{ir} + \gls{pr} & 0.00458 & 0.00526 & 0.0429 & 0.0538 & 0.162 & \textbf{0.195} & \textbf{0.177} & \textbf{0.187} \\
		\gls{lda}-\gls{ir} * \gls{pr} & 0.00781 & 0.00569 & 0.0410 & 0.0537 & 0.156 & 0.186 & 0.168 & 0.179 \\
		\gls{lm} + \gls{lda}-\gls{ir} & 0.0419 & 0.0214 & 0.0602 & 0.120 & 0.147 & 0.163 & 0.145 & 0.146 \\
		\gls{lm} * \gls{lda}-\gls{ir} & 0.0931 & 0.0462 & 0.175 & 0.177 & 0.150 & 0.175 & 0.155 & 0.166 \\
		\gls{lm} + \gls{pr} & 0.170 & 0.153 & 0.283 & 0.256 & 0.130 & 0.132 & 0.130 & 0.131 \\
		\gls{lm} * \gls{pr} & 0.163 & 0.138 & 0.259 & 0.236 & 0.130 & 0.133 & 0.129 & 0.130 \\
		\gls{lm} + \gls{lda}-\gls{ir} + \gls{pr} & 0.0499 & 0.0214 & 0.0601 & 0.120 & 0.148 & 0.164 & 0.146 & 0.147 \\
		\gls{lm} * \gls{lda}-\gls{ir} * \gls{pr} & 0.0911 & 0.0459  & 0.157 & 0.170 & 0.150 & 0.175 & 0.155 & 0.166 \\
		\gls{bm25} + \gls{lda}-\gls{ir} & \textbf{0.276} & 0.524 & 0.588 & 0.365 & 0.155 & 0.184 & 0.168 & 0.176 \\
		\gls{bm25} * \gls{lda}-\gls{ir} & 0.139 & 0.270 & 0.412 & 0.365 & 0.154 & 0.159 & 0.156 & 0.162 \\
		\gls{bm25} + \gls{pr} & 0.269 & 0.656 & 0.866 & 0.902 & \textbf{0.192} & 0.193 & 0.175 & 0.183 \\
		\gls{bm25} * \gls{pr} & 0.267 & \textbf{0.663} & \textbf{0.884} & 0.904 & 0.155 & 0.159 & 0.155 & 0.161 \\
		\gls{bm25} + \gls{lda}-\gls{ir} + \gls{pr} & \textbf{0.276} & 0.525 & 0.589 & 0.366 & 0.162 & 0.192 & 0.176 & 0.184 \\
		\gls{bm25} * \gls{lda}-\gls{ir} * \gls{pr} & 0.150 & 0.266 & 0.446 & 0.381 & 0.155 & 0.159 & 0.156 & 0.163 \\
	\end{tabular}
	
	\label{tab:results}
\end{table*}


\begin{table*}[h]
	\centering
	\caption{Precision results for the four sets of topic queries for P@10 and P@100.}
	\begin{tabular}{l|c|c|c|c||c|c|c|c}
		& \multicolumn{4}{c||}{Topic Query Length (P@10)} & \multicolumn{4}{c}{Topic Query Length (P@100)} \\
		Model & 1 & 2 & 3 & 4 & 1 & 2 & 3 & 4\\
		\midrule
		\gls{lda}-\gls{ir} & 0.02 & 0.103 & 0.13 & 0.203 & 0.062 & 0.131 & 0.164 & 0.191 \\
		\gls{lm} & 0.126 & 0.118 & 0.116 & 0.069 & 0.090 & 0.092 & 0.087 & 0.093\\
		\gls{bm25} & 0.136 & 0.161 & 0.164 & 0.174 & 0.142 & 0.165 & 0.175 & 0.151\\ 
		\gls{tf-idf} & 0.160 & 0.125 & 0.200 & 0.148 & 0.163 & 0.169 & \textbf{0.188} & 0.170 \\
		\gls{lda}-\gls{ir} + \gls{pr} & 0.0188 & 0.103 & 0.141 & \textbf{0.211} & 0.062 & 0.131 & 0.175 & \textbf{0.198} \\
		\gls{lda}-\gls{ir} * \gls{pr} & 0.0125 & 0.100 & 0.133 & 0.200 & 0.062 & 0.132 & 0.167 & 0.192 \\
		\gls{lm} + \gls{lda}-\gls{ir} & 0.02 & 0.101 & 0.136 & 0.196 & 0.060 & 0.129 & 0.161 & 0.188  \\
		\gls{lm} * \gls{lda}-\gls{ir} & 0.02 & 0.085 & 0.109 & 0.161 & 0.055 & 0.114 & 0.129 & 0.152 \\
		\gls{lm} + \gls{pr} & 0.138 & 0.130 & 0.116 & 0.0763 & 0.110 & 0.108 & 0.097 & 0.098 \\
		\gls{lm} * \gls{pr} & 0.148 & 0.128 & 0.116 & 0.0963 & 0.110 & 0.112 & 0.101 & 0.101 \\
		\gls{lm} + \gls{lda}-\gls{ir} + \gls{pr} & 0.019 & 0.101 & 0.134 & 0.195 & 0.061 & 0.129 & 0.163 & 0.187\\
		\gls{lm} * \gls{lda}-\gls{ir} * \gls{pr} & 0.026 & 0.090 & 0.110 & 0.161 & 0.059 & 0.115 & 0.130 & 0.152\\
		\gls{bm25} + \gls{lda}-\gls{ir} & 0.124 & 0.160 & 0.154 & 0.204 & 0.113 & 0.155 & 0.168 & \textbf{0.198} \\
		\gls{bm25} * \gls{lda}-\gls{ir} & 0.09 & 0.139 & 0.175& 0.206 & 0.134 & 0.170 & 0.174 & 0.187 \\
		\gls{bm25} + \gls{pr} & 0.135 & 0.163 & 0.165 & 0.173 & \textbf{0.155} & 0.165 & 0.176 & 0.151 \\
		\gls{bm25} * \gls{pr} & \textbf{0.165} & \textbf{0.169} & \textbf{0.184} & 0.170 & 0.148 & \textbf{0.177} & 0.186 & 0.161\\
		\gls{bm25} + \gls{lda}-\gls{ir} + \gls{pr} & 0.124 & 0.160 & 0.154 & 0.204 & 0.113 & 0.155 & 0.17 & \textbf{0.198}\\
		\gls{bm25} * \gls{lda}-\gls{ir} * \gls{pr} & 0.095 & 0.141 & 0.176 & 0.206 & 0.135 & 0.174 & 0.174 & 0.188\\
	\end{tabular}
	
	\label{tab:results_precision_at_10}
\end{table*}


\begin{table}[h]
	\centering
	\caption{Average rank of the relevant document for each document query.}
	\begin{tabular}{l|c|c|c|c}
		& \multicolumn{4}{c}{Document Query Length} \\
		Model / Avg. Rank & 1 & 2 & 3 & 4 \\
		\midrule
		\gls{lda}-\gls{ir} & 2287.93 & 1599.95 & 1241.18 & 1926.78 \\
		\gls{lm} & 7120.04 & 9082.9 & 6501.85 & 7782.65 \\
		\gls{bm25} & \textbf{19.58} & 7.94 & 1.78 & 1.41 \\
		\gls{tf-idf} & 30.0 & 9.3 & 2.03 & 1.29 \\
		\gls{lda}-\gls{ir} + \gls{pr} & 2491.31 & 1342.53 & 1126.23 & 1906.76\\
		\gls{lda}-\gls{ir} * \gls{pr} & 2305.04 & 1600.93 & 1223.14 & 1920.175\\
		\gls{lm} + \gls{lda}-\gls{ir} & 1971.19 & 1192.91 & 1027.95 & 1482.69 \\
		\gls{lm} * \gls{lda}-\gls{ir} & 1874.81 & 1456.21 & 954.66 & 1853.44 \\
		\gls{lm} + \gls{pr} & 7299.85 & 9134.81 & 6429.24 & 7725.36 \\
		\gls{lm} * \gls{pr} & 7328.7625 & 9137.23 & 6504.85 & 7772.4\\
		\gls{lm} + \gls{lda}-\gls{ir} + \gls{pr} & 1978.74 & 1179.21 & 994.91 & 1438.88 \\
		\gls{lm} * \gls{lda}-\gls{ir} * \gls{pr} & 1892.12 & 1453.56 & 941.4 & 1850.43 \\
		\gls{bm25} + \gls{lda}-\gls{ir} & 30.45 & 28.59 & 17.7 & 23.76 \\
		\gls{bm25} * \gls{lda}-\gls{ir} & 163.76 & 557.13 & 297.48 & 1159.33 \\
		\gls{bm25} + \gls{pr} & 19.69 & \textbf{7.88} & 1.79 & 1.43\\
		\gls{bm25} * \gls{pr} & 23.96 & 8.45 & \textbf{1.61} & \textbf{1.24}\\
		\gls{bm25} + \gls{lda}-\gls{ir} + \gls{pr} & 30.35 & 28.44 & 17.65 & 23.76\\
		\gls{bm25} * \gls{lda}-\gls{ir} * \gls{pr} & 163.5 & 555.35 & 295.69 & 1158.08\\
	\end{tabular}
	
	\label{tab:hit_results}
\end{table}
