\section{Experiments}\label{sec:experiment}

\subsection{Hyperparameters}\label{subsec:hyperparameters}
Before setting up our experiment, we did some initial testing to find acceptable hyperparameter values for our \gls{lda} model.
% runtime parameters
First, we adjusted parameters that had to do with how the algorithms run to ensure that the model converged, without spending unnecessary resources.
\texttt{passes} adjusts how many times the model passes over the whole corpus, and can be seen as an 'epoch'. 
This is necessary for smaller datasets to ensure convergence before the algorithm finishes, but for large enough datasets this parameter can have a value of 1, as is the case for \cite{blei2010online} where their corpus includes 3.3M documents, where our dataset is $\sim$32000 documents.
\texttt{iterations} which adjusts the maximum number of iterations of the E-step (from \cite{blei2010online}, Algorithm 2) that are allowed without all documents having converged. 
Setting this to a high value ensures convergence, but also increases training time. 
In practice, iterations should be set to the lowest possible values where nearly all (> 99\%) documents have converged by the end of the training.
From testing we found 20 \texttt{passes} and 100 \texttt{iterations} was enough to converge 27678 out of 27919 documents (99.14\%).

% model parameters
Once we had assured convergence of our model, we tested hyperparameters for our \gls{lda} model.
These include:
\begin{itemize}
	\item K - the number of topics
	\item $\alpha$ - dirichlet prior for document-topic distributions
	\item $\eta$ - dirichlet prior for topic-word distributions
\end{itemize}
Similar to clustering algorithms, there is usually an optimal setting for K, where all values above will produce topics that are subtopics of the optimal topics, and all values below will achieve subpar performance.
This theoretical optimal value is however based on the corpus, so tests are needed to find some setting of K that is close to the optimal.
$\alpha$ and $\eta$ adjusts the Dirichlet distributions, which will create the multinomial distributions for document-topic and topic-word relations, respectively.
Lower values will lead to more uneven distributions, favoring fewer topics per document and fewer words per topic, while higher values will make the distribution more uniform.

To find the optimal hyperparameter values we did two sequential grid-searches using the hyperparameter values shown in \autoref{tab:params}.
The first grid-search used the values for $K_1$, while the second used the $K_2$ values.
In both grid-searches the same $\alpha$ and $\eta$ values are used.
To evaluate the \gls{lda} models, we measured perplexity \todo{explain perplexity}\todo{and explain how we choose our model by hand}.
During the first grid-search, we found that $10$ and $50$ were clearly the best values for K.
However, $\alpha$ and $\eta$ values showed no clear favorite.
For the second grid-search we choose more specific K values, to narrow the search, while keeping the $\alpha$ and $\eta$ values the same.
\todo[inline]{add graphs from gridsearches}

Based on manual inspection and evaluation of topics generated by the second grid-search we choose $K = 30$, $\alpha = 0.1$, and $\eta = 0.1$ as the hyperparameters for the final \gls{lda} model.
A new model is generated using these values and this model will be used for the whole experiment.

\begin{table}[h]
	\centering
	\begin{tabular}{c|c}
		Parameter & Tested Values\\
		\hline
		$K_1$ & 10, 50, 100, 200, 300\\
		$K_2$ & 5, 10, 15, 20, 25, 30, 35, 40, 45, 50\\
		$\alpha$ & 0.5, 0.1, 0.01, 0.001\\
		$\eta$ & 0.1, 0.01, 0.001, 0.0001\\
	\end{tabular}
	\caption{Tested hyperparameter values. $K_1$ is the $K$ values used for the first hyperparameter test, while $K_2$ is the $K$ values used for the second hyperparameter test.}
	\label{tab:params}
\end{table}
