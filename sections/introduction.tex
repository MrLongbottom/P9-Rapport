\section{Introduction} 
%% Argue briefly for the relevance of the studied area/problem â€” start from general and end with your concrete problem.


Many news articles are produced every day and the need for searching the news is becoming a more prominent and difficult task.
A general challenge for search engines, no matter the environment, is how to acquire relevant search results from a given corpus. 
While some search engines use simple algorithms, such as finding all articles that include a given search term and sorting by date, this may not always provide useful results.

\gls{ir} within search engines apply many different techniques for either ranking or filtering documents\cite{google_pagerank2006}.
In order to rank these documents, some information is needed to support the underlying ranking.
A popular way to extract this information is through the use of topic modeling.
Topic modeling is a method within machine learning and \gls{NLP} which builds a model that can find abstract topics within a corpus of text documents.
A topic model assigns a distribution of topics to each document in a corpus, which can be used for different applications.
More specifically, we look into \gls{lda} since it is currently the most used topic model, and is applicable for many situations\cite{lda}.
\gls{lda} uses Dirichlet distributions to calculate the probability of a word belonging to a given topic and a document containing a given topic.
Topic modeling does not rank documents, which is why an additional part is necessary if we want to search the documents.

Ranking articles is a common \gls{ir} task and has been done in many ways.
One commonly used method is \gls{pr}\cite{google_pagerank2006}.
\gls{pr}\cite{pagerank_1999} is a ranking algorithm, which is based on a random walk process.
The principle is to let a number of surfers walk between nodes in a graph, where each surfer takes one step in each iteration.
After an appropriate number of iterations, the number of surfers at each node will be the ranking of the given node, where more surfers give a higher rank.
The edges in the graph can be based on many types of interactions, such as hyperlinks, similarities, genres, etc.

\citeauthor{yang2009topic}\cite{yang2009topic} investigate how to combine topic model \gls{lda}, \gls{lm}, and \gls{pr}.
They also investigate a modified version of the \gls{pr} algorithm which takes topic-level information into account.
They use a scientific article dataset where they manually label 200 articles from their dataset based on related queries. 
In this paper, we want to expand the framework which they have provided.
However, the dataset which \cite{yang2009topic} uses is based on scientific articles, which includes citations to other similar articles.
This citation scheme does not necessarily extent to other document types, such as news articles.
We also want to investigate whether using a similarity measure based on the topics generated by a topic model can be used with \gls{pr} to improve \gls{ir} methods.

The combination of \gls{lda} and \gls{pr} is interesting, since underlying document topics can yield more information than usually given by a single document.
These topics can be seen as sets of words shared between documents. 

When evaluating search engines, manual labeling of documents is usually used to measure the performance of models\cite{yang2009topic}\cite{Tang2008}.
Since the topics, created by the \gls{lda}, are distributions of words in documents, we can use these to evaluate a search algorithm's retrieval performance. 

Our goal is thus to improve news article search by using a combination of \gls{lda} and \gls{pr}.
For our experiments we use the Nordjyske database of news articles as our dataset.

We also want to investigate how an automatic evaluation measure can be used to evaluate \gls{ir} methods.
To evaluate \gls{ir} methods, a method for generating queries is made.
Here we put the focus on the \gls{ir} method interpreting the underlying topics expressed in a query.

Finally, since our news article dataset does not include connections between the articles, we want to investigate how we can build an adjacency matrix based on the documents themselves, in order to run \gls{pr}.
With these goals we look at the following questions:
\begin{quote}
	\begin{itemize}
		\item \emph{How can we generate queries for a dataset to use for \gls{ir}?}
		\item \emph{How can we evaluate \gls{ir} methods in a way that favors abstraction, rather than word frequency?}
	        \item \emph{Can \gls{pr} be used on a document dataset with no explicit connections to improve \gls{ir} methods?}
	\end{itemize}
\end{quote}

\input{sections/introduction/contributions}

\input{sections/introduction/structure}
