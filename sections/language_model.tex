\section{Language model}
The \gls{lda} model gets poor document retrieval performance when not used in combination with another model\cite{yang2009topic}.
In \cite{yang2009topic}, they describe various combinations of \gls{lda} and other models. 
The language model they describe is similar to a query likelihood model, which generates a probability of how likely a given document $d$ produces a given query $q$.
To calculate this probability, they find the likelihood for each word in the query by using this function:
\begin{equation}\label{eq:word_prob}
	P(w|d) = \frac{N_d}{N_d + \lambda} \cdot \frac{tf(w,d)}{N_d} + (1 - \frac{N_d}{N_d + \lambda}) \cdot \frac{tf(w,D)}{N_D}
\end{equation}

where $N_d$ is the number of word tokens in $d$ and $tf(w,d)$ is the word frequency of $w$ in $d$. $\lambda$ is a Dirichlet smoothing factor and is set to the average document length.
When $D$ is used, it is referring to the document corpus.
$ \frac{N_d}{N_d + \lambda} $ describes a relative term which is weighted based on $ \lambda $. If $N_d > \lambda$ the value is closer to 1 and if $ N_d < \lambda $ the value is closer to 0.
$ \frac{tf(w,d)}{N_d} $ and $\frac{tf(w,D)}{N_D}$ are the maximum likelihood estimate of $w$ appearing in a document $d$ or the corpus $D$, respectively.
\autoref{eq:word_prob} calculates the relative maximum likelihood for the word being in $ d $ and $ D $, and weighs these terms based on the document length $N_d$ compared to $\lambda$, with smaller documents weighing the corpus likelihood higher, and bigger documents weighing the document likelihood higher.

To calculate this for a given query, we take the product of the word likelihood to get the probability for $q$.

\begin{equation}\label{eq:query_prob}
	P(q|d) = \prod_{w \in q} P(w|d)
\end{equation}

\cite{yang2009topic} combine the \gls{lda} model and the language model to get information about the topic and word correlation between $q$ and $d$.
