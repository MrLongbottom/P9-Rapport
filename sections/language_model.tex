\subsection{Language model}\label{sec:lm}
In \cite{yang2009topic}, \citeauthor{yang2009topic} describe various combinations of \gls{lda} and other models. 
The \acrlong{lm} they describe is similar to a query likelihood model, which generates a probability of how likely a given query $q$ is generated by a given document $d$.
To calculate this probability, they find the likelihood for each word in the query by using the following equation:
\begin{equation}\label{eq:word_prob}
	P(w|d) = \frac{N_d}{N_d + \lambda} \cdot \frac{tf(w,d)}{N_d} + (1 - \frac{N_d}{N_d + \lambda}) \cdot \frac{tf(w,D)}{N_D}
\end{equation}

where $N_d$ is the number of word tokens in document $d$ and $tf(w,d)$ is the word frequency of word $w$ in document $d$. Likewise, $N_D$ is the total number of word tokens in the corpus $D$, and $tf(w,D)$ is the word frequency of word $w$ in the whole corpus $D$. $\lambda$ is a Dirichlet smoothing factor and is set to the average document length.
$ \frac{N_d}{N_d + \lambda} $ describes a relative term which is weighted based on $ \lambda $. If $N_d > \lambda$ the value is closer to 1 and if $ N_d < \lambda $ the value is closer to 0.
$\frac{tf(w,d)}{N_d}$ is the maximum likelihood estimate of $w$ appearing in $d$.
$\frac{tf(w,D)}{N_D}$ is the maximum likelihood estimate of $w$ in the corpus $D$.
\autoref{eq:word_prob} calculates the relative maximum likelihood for the word being in $ d $ and $ D $, and weighs these terms based on the document length $N_d$ compared to $\lambda$.
Documents smaller than $N_d$, favours the term frequency of $w$ in corpus higher, and documents that are bigger than $N_d$ is favoured if they have a high term frequency of $w$.
So there is always a constant trade off between these two sizes of documents based on the term frequency.

\cite{yang2009topic} combine the \gls{lda} model and the \gls{lm} to get information about the topic and word correlation between $q$ and $d$.
